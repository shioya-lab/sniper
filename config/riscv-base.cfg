[general]
arch = riscv
enable_icache_modeling = true
mode = 64
syntax = default
magic = true
konata_count_max = 1000000
#total_cores = 2
inst_mode_init = fast_forward
inst_mode_roi = detailed
inst_mode_end = fast_forward
inst_mode_output = true

[perf_model/core]
core_model = riscv
frequency = 2
type = rob # or interval
logical_cpus = 1 # Number of SMT threads per core

[perf_model/core/rob_timer]   # https://github.com/ucb-bar/riscv-boom/blob/master/src/main/scala/common/configs.scala
address_disambiguation = true  # Allow loads to bypass preceding stores with an unknown address
commit_width = 10                      # Commit bandwidth (instructions per cycle), per SMT thread
issue_contention = true
issue_memops_at_issue = true  # Issue memops to the memory hierarchy at issue time (false = before dispatch)
mlp_histogram = false
outstanding_loads = 12
outstanding_stores = 12
outstanding_vec_loads = 32
outstanding_vec_stores = 32
rob_repartition = true
rs_entries = 36
simultaneous_issue = true
store_to_load_forwarding = true
gather_scatter_merge = true

[perf_model/core/interval_timer]
dispatch_width = 4
window_size = 152   # 64 + 24 + 32 + 32 = 152
alu_window_size = 64
lsu_window_size = 24
fpu_window_size = 32
vec_window_size = 32
num_outstanding_loadstores = 24

[perf_model/core/static_instruction_costs]
add = 1
sub = 1
mul = 3
div = 12  # 4-12 Word, 4-20 64-bit
fadd = 4
fsub = 4
fmul = 4
fdiv = 11  # 6-11 s form, 6-18 d form
generic = 1
jmp = 1
string = 1
branch = 1
dynamic_misc = 1
recv = 1
sync = 0
spawn = 0
tlb_miss = 0
mem_access = 0
delay = 0
unknown = 0

[perf_model/branch_predictor]
type = pentium_m
mispredict_penalty = 15
size = 1024  # ??

#https://github.com/ucb-bar/riscv-boom/blob/master/src/main/scala/common/configs.scala
[perf_model/cache]
levels = 2

[perf_model/tlb]
penalty = 30   # "variable number of cycles"

[perf_model/itlb]
size = 32 # Number of I-TLB entries
associativity = 32 # full associative

[perf_model/dtlb]
size = 32 # Number of D-TLB entries
associativity = 32 # fully associative

[perf_model/stlb]
size = 1024 # Number of second-level TLB entries
associativity = 4 # S-TLB associativity

[perf_model/l1_icache]
cache_block_size = 64 # in B
cache_size = 32 # in KB
associativity = 4
replacement_policy = lru
writethrough = 0
perfect = false
passthrough = false
coherent = true
data_access_time = 3
tags_access_time = 1
perf_model_type = parallel
writeback_time = 0    # Extra time required to write back data to a higher cache level
dvfs_domain = core    # Clock domain: core or global
shared_cores = 1      # Number of cores sharing this cache
next_level_read_bandwidth = 0 # Read bandwidth to next-level cache, in bits/cycle, 0 = infinite
prefetcher = simple
address_hash = mask

[perf_model/l1_icache/prefetcher]
prefetch_on_prefetch_hit = false
train_prefetcher_on_hit = false
delay_prefetcher = false

[perf_model/l1_icache/prefetcher/simple]
flows = 8
flows_per_core = false
num_prefetches = 32
stop_at_page_boundary = true


[perf_model/l1_dcache]
cache_block_size = 64
num_banks = 4
cache_size = 32 # in KB
associativity = 8
replacement_policy = lru
# prefetcher = none
prefetcher = simple
# prefetcher = stream  #simple  #??
writethrough = 0
perfect = false
passthrough = false
address_hash = mask
data_access_time = 2
tags_access_time = 1
perf_model_type = parallel
writeback_time = 0    # Extra time required to write back data to a higher cache level
dvfs_domain = core    # Clock domain: core or global
shared_cores = 1      # Number of cores sharing this cache
outstanding_misses = 8
next_level_read_bandwidth = 0 # Read bandwidth to next-level cache, in bits/cycle, 0 = infinite

[perf_model/l1_dcache/prefetcher]
prefetch_on_prefetch_hit = false
train_prefetcher_on_hit = false
delay_prefetcher = false

[perf_model/l1_dcache/prefetcher/simple]
flows = 8
flows_per_core = false
num_prefetches = 64
stop_at_page_boundary = true

[perf_model/l1_dcache/prefetcher/stream]
table_size = 32
degree = 8
distance = 8
training_window = 4
training_threshold = 0

[perf_model/l2_cache]
cache_block_size = 64 # in bytes
cache_size = 512 # in KB
associativity = 16
replacement_policy = lru
data_access_time = 8 # Maximum from the overall RAM latency calculation table, tech ref manual
tags_access_time = 5 # Maximum from the overall RAM latency calculation table, tech ref manual
writeback_time = 2 # approx of 1.5 ns; 3 cycles at 2 GHz; Extra time required to write back data to a higher cache level
# prefetcher = none # optional
prefetcher = simple # optional
# prefetcher = stream # optional
next_level_read_bandwidth = 1 # 2 cycles at 2 GHz; Read bandwidth to next-level cache, in bits/cycle, 0 = infinite
writethrough = 0
perfect = false
passthrough = false
address_hash = mask
dvfs_domain = core    # Clock domain: core or global
shared_cores = 1      # Number of cores sharing this cache
outstanding_misses = 16
perf_model_type = parallel

[perf_model/l2_cache/prefetcher]
prefetch_on_prefetch_hit = false

[perf_model/l2_cache/prefetcher/simple]
flows = 64
flows_per_core = true
num_prefetches = 128
stop_at_page_boundary = true

[perf_model/l2_cache/prefetcher/stream]
table_size = 64
degree = 64
distance = 64
training_window = 8
training_threshold = 0

[perf_model/l3_cache]
cache_block_size = 64 # in bytes
cache_size = 512 # in KB
associativity = 16
replacement_policy = lru
data_access_time = 8 # Maximum from the overall RAM latency calculation table, tech ref manual
tags_access_time = 5 # Maximum from the overall RAM latency calculation table, tech ref manual
writeback_time = 2 # approx of 1.5 ns; 3 cycles at 2 GHz; Extra time required to write back data to a higher cache level
# prefetcher = none # optional
prefetcher = none
# prefetcher = stream # optional
next_level_read_bandwidth = 1 # 2 cycles at 2 GHz; Read bandwidth to next-level cache, in bits/cycle, 0 = infinite
writethrough = 0
perfect = true
passthrough = false
address_hash = mask
dvfs_domain = core    # Clock domain: core or global
shared_cores = 1      # Number of cores sharing this cache
outstanding_misses = 16
perf_model_type = parallel

[perf_model/llc]
evict_buffers = 20 # minimum from the manual: "Configurable number of Fill/Eviction Queue (FEQ) entries to 20, 24, or 28."

[caching_protocol]
type = parametric_dram_directory_msi
variant = mesi # msi, mesi or mesif

[perf_model/dram_directory]
total_entries = 16384
associativity = 16
directory_type = full_map # Supported (full_map, limited_no_broadcast, limitless)

[perf_model/dram]
type = constant # DRAM performance model type: "constant" or a "normal" distribution
latency = 50    # In nanoseconds
per_controller_bandwidth = 5 # In GB/s
num_controllers = -1 # Total Bandwidth = per_controller_bandwidth * num_controllers
controllers_interleaving = 1 # If num_controllers == -1, place a DRAM controller every N cores
chips_per_dimm = 8
dimms_per_controller = 4
controller_positions = ""
direct_access = true                     # Access DRAM controller directly from last-level cache (only when there is a single LLC)


[perf_model/sync]
reschedule_cost = 0 # In nanoseconds

[network/bus]
ignore_local_traffic = true # Do not count traffic between core and directory on the same tile
bandwidth = 25.6 # in GB/s. Actually, it's 12.8 GB/s per direction and per connected chip pair

[dvfs]
type = simple
transition_latency = 0 # In nanoseconds
